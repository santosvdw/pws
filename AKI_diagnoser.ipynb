{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4aG7qte_ZJS"
      },
      "source": [
        "# PWS: Machine learning in de zorg\n",
        "Santos van der Wansem en Mina Qayumzada\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> Acuut nierfalen is een complexe aandoening die vaak voorkomt bij volwassenen op de intensive care-afdeling. Acuut nierfalen wordt gekenmerkt door een abrupte (binnen enkele uren) afname van de nierfunctie, wat zowel structurele schade aan de nieren als functieverlies inhoudt. Dit leidt tot een ophoping van afvalproducten in het bloed en verhindert de nieren om de juiste vochtbalans in het lichaam te handhaven. Acuut nierfalen is een ernstig syndroom vanwege de gerelateerde morbiditeit en mortaliteit. Er zijn echter een paar markers voor acuut nierfalen, deze markers zullen voor dit machine learning model de basis vormen voor de manier waarop het model de voorspellingen zal doen.\n",
        "\n",
        "> Met dank aan Jerina Vries en Sonja Steiger, onze begeleiders"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usage\n",
        "\n",
        "This model consists of three parts:\n",
        "\n",
        "- The first part is where the data from the MIMIC-iii dataset gets transformed into a dataset that can be interpreted by the machine learning algorithms. If you do not have access to the MIMIC-iii database, you can skip this step and simply download a subset of the MIMIC-iii database, that is ready to be used for training the machine learning algorithm. Also please note, that if you do have access to the MIMIC-iii dataset, you should change the paths in the code to the path to your own MIMIC-iii files. On initiation, all file paths will be set to a certain value that may not correspond to the correct location of your MIMIC-iii files.\n",
        "- The second part is where the data is used to train a machine learning model. There are currently five algorithms that get trained with the data: K-nearest-neighbors, Naive bayes, Logistic regression, Support vector machine and a Neural network.\n",
        "- The third part is where the trained model (in this case the KNN model but you may select a different model) gets downloaded, and then loaded to showcase to the user how this model can be implemented in other environments.\n",
        "\n",
        "> To gain access to an already trained version of the KNN algorithm, or to the pre-made dataset, simply head over to the github of [@santosvdw](https://github.com/santosvdw/pws) and download the desired files."
      ],
      "metadata": {
        "id": "vsLfJOJyvL92"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjQm2poa_1m4"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "This model uses the MIMIC-iii dataset. MIMIC-III is a large, freely-available database comprising deidentified health-related data associated with over forty thousand patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012. The database includes information such as demographics, vital sign measurements made at the bedside (~1 data point per hour), laboratory test results, procedures, medications, caregiver notes, imaging reports, and mortality (including post-hospital discharge).\n",
        "\n",
        "Johnson, A., Pollard, T., & Mark, R. (2016). MIMIC-III Clinical Database (version 1.4). PhysioNet. https://doi.org/10.13026/C2XW26"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTlvaFs5TbqM"
      },
      "source": [
        "# Data selection/manipulation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "import pandas as pd\n",
        "import math\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from datetime import datetime, date"
      ],
      "metadata": {
        "id": "uLvDlxyVvQoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl36vXAjtPC-"
      },
      "source": [
        "## Selecting relevant variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XLJaN4dJAo5"
      },
      "outputs": [],
      "source": [
        "# ------------\n",
        "# Selecting relevant variables\n",
        "# ------------\n",
        "\n",
        "# Select all diagnoses events\n",
        "df = pd.read_csv('/content/drive/MyDrive/MIMIC/DIAGNOSES_ICD.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ws-DHVy4D3A9"
      },
      "outputs": [],
      "source": [
        "# Select all diagnoses events\n",
        "df = pd.read_csv('/content/drive/MyDrive/MIMIC/DIAGNOSES_ICD.csv')\n",
        "\n",
        "# Manually selected AKI related codes\n",
        "aki_codes = [\"66930\",\"66932\", \"66934\",\"5845\",\"5846\",\"5847\",\"5848\",\"5849\"]\n",
        "\n",
        "# Select all AKI related diagnoses\n",
        "aki = df.loc[df[\"ICD9_CODE\"].isin(aki_codes)]\n",
        "\n",
        "# Select all patients with AKI related diagnosis\n",
        "patients = []\n",
        "for index, row in aki.iterrows():\n",
        "  patients.append(row['SUBJECT_ID'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SnATZjdD80L"
      },
      "outputs": [],
      "source": [
        "# ------------\n",
        "# Select all lab events for AKI-diagnosed patients\n",
        "# ------------\n",
        "data = pd.read_csv('/content/drive/MyDrive/MIMIC/LABEVENTS.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtDKf8gBEH_D"
      },
      "outputs": [],
      "source": [
        "# Select all measurements of AKI-related lab variables\n",
        "markers = [50912,51006,43175]\n",
        "data = data.loc[data['ITEMID'].isin(markers)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKN-2fiStlLg"
      },
      "source": [
        "## Transforming the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibnWDvAfYiIT"
      },
      "outputs": [],
      "source": [
        "# ------------\n",
        "# Transforming the dataset\n",
        "# ------------\n",
        "\n",
        "# Dropping incomplete and irrelevant data\n",
        "data = data[data['HADM_ID'].notna()]\n",
        "data = data.drop(columns=[\"VALUE\", \"CHARTTIME\", \"FLAG\"])\n",
        "\n",
        "# OPTIONAL BUT RECOMMENDED: SELECT A SUBSET OF THE\n",
        "# DATASET TO SPEED UP THE TRANSFORMATIONPROCESS\n",
        "# (OTHERWISE IT MAY TAKE HOURS)\n",
        "\n",
        "# data = data.head(400000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Fj8kcxIaULk"
      },
      "outputs": [],
      "source": [
        "# ------------\n",
        "# Selecting vector variables\n",
        "# ------------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rotating table on its axis\n",
        "data[markers] = -1\n",
        "data = data.drop(columns=[\"VALUEUOM\", \"ROW_ID\"])"
      ],
      "metadata": {
        "id": "2jWZHsyEshcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_TYvCNMKzpq"
      },
      "outputs": [],
      "source": [
        "# ------------\n",
        "# Selecting urine output\n",
        "# ------------\n",
        "out = pd.read_csv('/content/drive/MyDrive/MIMIC/OUTPUTEVENTS.csv')\n",
        "out = out.drop(columns=['ICUSTAY_ID', \"CHARTTIME\", \"STORETIME\", 'CGID', 'STOPPED', \"NEWBOTTLE\", \"ISERROR\"])\n",
        "\n",
        "# Select urine output from file\n",
        "out = out.loc[out['ITEMID']==43175]\n",
        "out = out.loc[out['VALUE'].notna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfSysTiiK6KC"
      },
      "outputs": [],
      "source": [
        "# Add all registered urine outputs to data table\n",
        "for index, row in out.iterrows():\n",
        "  new = [row['SUBJECT_ID'], row['HADM_ID'], 43175,0,-1,-1, row[\"VALUE\"]]\n",
        "  data.loc[len(data.index)+1] = new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYkOtvfj8lo0"
      },
      "outputs": [],
      "source": [
        "# Selecting values for serum creatinine and blood urea nitrogen\n",
        "for i in markers:\n",
        "  for index, row in data.iterrows():\n",
        "    if row[\"ITEMID\"] == i:\n",
        "      data.loc[data[\"HADM_ID\"] == row[\"HADM_ID\"], i] = row[\"VALUENUM\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRDozyz0RPxM"
      },
      "outputs": [],
      "source": [
        "# Adding output value column \"AKI\"\n",
        "data[\"AKI\"] = 0\n",
        "data.loc[data[\"SUBJECT_ID\"].isin(patients), \"AKI\"] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVv75F8TlDdE"
      },
      "outputs": [],
      "source": [
        "# ------------\n",
        "# Calculating age\n",
        "# ------------\n",
        "\n",
        "# Read admissions file and drop irrelevant columns\n",
        "adm = pd.read_csv('/content/drive/MyDrive/MIMIC/ADMISSIONS.csv')\n",
        "adm = adm.drop(columns=['DEATHTIME', 'DISCHTIME', 'ADMISSION_LOCATION', 'DISCHARGE_LOCATION', 'INSURANCE', 'LANGUAGE',\n",
        "                        'RELIGION', 'HOSPITAL_EXPIRE_FLAG','HAS_CHARTEVENTS_DATA', 'EDREGTIME', 'EDOUTTIME', 'MARITAL_STATUS'])\n",
        "# Read patients file and drop irrelevant columns\n",
        "pat = pd.read_csv('/content/drive/MyDrive/MIMIC/PATIENTS.csv')\n",
        "pat = pat.drop(columns=['DOD', 'DOD_HOSP', 'DOD_SSN', \"EXPIRE_FLAG\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ao270CfSlGXv"
      },
      "outputs": [],
      "source": [
        "# Register time of birth in data table\n",
        "for index, row in pat.iterrows():\n",
        "  data.loc[data['SUBJECT_ID'] == row['SUBJECT_ID'], 'DOB'] = row['DOB']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EO_qfgF0rnCY"
      },
      "outputs": [],
      "source": [
        "# Register time of admission in data table\n",
        "for index, row in adm.iterrows():\n",
        "  data.loc[data['SUBJECT_ID'] == row['SUBJECT_ID'], 'ADMITTIME'] = row['ADMITTIME']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrFxL4fblIwS"
      },
      "outputs": [],
      "source": [
        "# Register age in patients table\n",
        "for index, row in data.iterrows():\n",
        "  age = rdelta = relativedelta(datetime.strptime(row['ADMITTIME'], '%Y-%m-%d %H:%M:%S').date(), datetime.strptime(row['DOB'],\n",
        "                                                                                                                  '%Y-%m-%d %H:%M:%S').date())\n",
        "  if age.years > 100:\n",
        "    age.years = 90\n",
        "  data.loc[data[\"SUBJECT_ID\"] == row[\"SUBJECT_ID\"], \"AGE\"] = age.years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCN3D2gLsZmW"
      },
      "outputs": [],
      "source": [
        "# Drop infant patients\n",
        "neo = data.loc[data['AGE'] < 5].index\n",
        "data = data.drop(neo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyDbHGZjARDQ"
      },
      "outputs": [],
      "source": [
        "# Dropping irrelevant data\n",
        "data = data.drop_duplicates(subset=\"HADM_ID\", keep='last')\n",
        "data = data.drop(columns=[\"HADM_ID\", \"ITEMID\", \"VALUENUM\", \"SUBJECT_ID\", \"ADMITTIME\", \"DOB\"])\n",
        "\n",
        "# Rearranging columns\n",
        "cols = list(data.columns.values)\n",
        "data = data[cols[0:3]+[cols[4]]+[cols[3]]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcSuET6YvQv1"
      },
      "outputs": [],
      "source": [
        "# ------------\n",
        "# OPTIONAL: DOWNLOAD SUBSET OF DATA FOR FUTURE USE (SAVES TIME)\n",
        "# ------------\n",
        "\n",
        "# data.to_csv('dataset.csv', encoding='utf-8', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJNahIQ0G9io"
      },
      "source": [
        "# Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGA3hIJMH6-v"
      },
      "outputs": [],
      "source": [
        "# ------------\n",
        "# Creating the model\n",
        "# ------------\n",
        "\n",
        "# Install packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlvaS1a05Jtb"
      },
      "outputs": [],
      "source": [
        "# ------------\n",
        "# OPTIONAL: ONLY ENABLE IF YOU CHOSE TO DOWNLOAD A SUBSET OF THE DATA\n",
        "# ------------\n",
        "\n",
        "# data = pd.read_csv('data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CepBY4-eHDmx"
      },
      "outputs": [],
      "source": [
        "# Reviewing the data\n",
        "cols = list(data.columns.values)\n",
        "for label in cols[:-1]:\n",
        "  plt.hist(data[data[\"AKI\"]==1][label], color=\"blue\", label=\"AKI\", alpha=0.7, density=True)\n",
        "  plt.hist(data[data[\"AKI\"]==0][label], color=\"red\", label=\"NO AKI\", alpha=0.7, density=True)\n",
        "  plt.ylabel(\"Probability\")\n",
        "  plt.xlabel(label)\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etdMIToeMBhN"
      },
      "source": [
        "## Splitting the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KB1eqbPfLYSQ"
      },
      "outputs": [],
      "source": [
        "# ------------\n",
        "# Splitting the dataset\n",
        "# ------------\n",
        "train, valid, test = np.split(data.sample(frac=1), [int(0.6*len(data)), int(0.8*len(data))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyMVJlwkL7pi"
      },
      "outputs": [],
      "source": [
        "def scale_dataset(dataframe, oversample=False):\n",
        "  x = dataframe[dataframe.columns[:-1]].values\n",
        "  y = dataframe[dataframe.columns[-1]].values\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  x = scaler.fit_transform(x)\n",
        "\n",
        "  if oversample:\n",
        "    ros = RandomOverSampler()\n",
        "    x, y = ros.fit_resample(x, y)\n",
        "\n",
        "  data = np.hstack((x, np.reshape(y, (-1,1))))\n",
        "\n",
        "  return data, x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7pyG7iSM7Wn"
      },
      "outputs": [],
      "source": [
        "# Create training, validation and test sets\n",
        "train, x_train, y_train = scale_dataset(data, oversample=True)\n",
        "valid, x_valid, y_valid = scale_dataset(data, oversample=False)\n",
        "test, x_test, y_test = scale_dataset(data, oversample=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OGQRlKr0jRG"
      },
      "outputs": [],
      "source": [
        "# Get rid of NaN values\n",
        "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "\n",
        "# Transforming NaN values\n",
        "imp.fit(x_train)\n",
        "x_train = imp.transform(x_train)\n",
        "\n",
        "imp.fit(x_test)\n",
        "x_test = imp.transform(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dxhndtPycCY"
      },
      "source": [
        "## kNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Iga1MyjydyC"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwJjNGv-ykDT"
      },
      "outputs": [],
      "source": [
        "# ------------\n",
        "# Create KNN model\n",
        "# ------------\n",
        "knn_model = KNeighborsClassifier(n_neighbors=1)\n",
        "knn_model.fit(x_train, y_train)\n",
        "y_pred = knn_model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XR3ed2cRzI1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62645848-2bb2-4e8d-e0df-fe29b9638499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n=1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99      2680\n",
            "           1       0.96      0.97      0.97      1190\n",
            "\n",
            "    accuracy                           0.98      3870\n",
            "   macro avg       0.98      0.98      0.98      3870\n",
            "weighted avg       0.98      0.98      0.98      3870\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ------------\n",
        "# Print model metrics\n",
        "# ------------\n",
        "\n",
        "print(\"n=1\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# False positives and true positive rates\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
        "\n",
        "# Calculate AUC\n",
        "auc = metrics.roc_auc_score(y_test, y_pred)\n",
        "print(auc)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
        "plt.title(f\"K Nearest Neighbors, n=1\")\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnMKODE_elPz"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XWeywwieooB"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import  GaussianNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxwX2nrZeveg"
      },
      "outputs": [],
      "source": [
        "# ------------\n",
        "# Create NB model\n",
        "# ------------\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(x_train, y_train)\n",
        "y_pred = nb_model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------\n",
        "# Print model metrics\n",
        "# ------------\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# False positives and true positive rates\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
        "\n",
        "# Calculate AUC\n",
        "auc = metrics.roc_auc_score(y_test, y_pred)\n",
        "print(auc)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
        "plt.title(\"Naive Bayes\")\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s3HqkC4qwG9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI3p2jO87tNK"
      },
      "source": [
        "## Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RF6Opx17vGs"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY3SWMKA7z9C"
      },
      "outputs": [],
      "source": [
        "# ------------\n",
        "# Create Logistic Regression model\n",
        "# ------------\n",
        "\n",
        "lg_model = LogisticRegression()\n",
        "lg_model = lg_model.fit(x_train, y_train)\n",
        "y_pred = lg_model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------\n",
        "# Print model metrics\n",
        "# ------------\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# False positives and true positive rates\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
        "\n",
        "# Calculate AUC\n",
        "auc = metrics.roc_auc_score(y_test, y_pred)\n",
        "print(auc)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
        "plt.title(\"Logistic Regression\")\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9A7dSLlmwyBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxvoKLLB8iG3"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwG4KwGj8jom"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-0pj9Yn8rIt"
      },
      "outputs": [],
      "source": [
        "# ------------\n",
        "# Create Logistic Regression model\n",
        "# ------------\n",
        "\n",
        "svm_model = SVC()\n",
        "svm_model = svm_model.fit(x_train, y_train)\n",
        "y_pred = svm_model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------\n",
        "# Print model metrics\n",
        "# ------------\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# False positives and true positive rates\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
        "\n",
        "# Calculate AUC\n",
        "auc = metrics.roc_auc_score(y_test, y_pred)\n",
        "print(auc)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
        "plt.title(\"Support Vector Machine\")\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PumR0OvAx9Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtIdG1r39ScJ"
      },
      "source": [
        "## Neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKdzZez89SIQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqYrXxMf-ZvS"
      },
      "outputs": [],
      "source": [
        "# Plot NN history\n",
        "def plot_history(history):\n",
        "  fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4))\n",
        "  ax1.plot(history.history['loss'], label='loss')\n",
        "  ax1.plot(history.history['val_loss'], label='val_loss')\n",
        "  ax1.set_xlabel('Epoch')\n",
        "  ax1.set_ylabel('Binary crossentropy')\n",
        "  ax1.grid(True)\n",
        "\n",
        "  ax2.plot(history.history['accuracy'], label='accuracy')\n",
        "  ax2.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "  ax2.set_xlabel('Epoch')\n",
        "  ax2.set_ylabel('Accuracy')\n",
        "  ax2.grid(True)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IajGsde_9ZPZ"
      },
      "outputs": [],
      "source": [
        "# Train NN model\n",
        "def train_model(x_train, y_train, num_nodes, dropout_prob, lr, batch_size, epochs):\n",
        "  nn_model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(num_nodes, activation='relu', input_shape=(4,)),\n",
        "      tf.keras.layers.Dropout(dropout_prob),\n",
        "      tf.keras.layers.Dense(num_nodes, activation='relu'),\n",
        "      tf.keras.layers.Dropout(dropout_prob),\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  nn_model.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  history = nn_model.fit(x_train, y_train, epochs=125, batch_size=32, validation_split=0.2, verbose=0)\n",
        "\n",
        "  return nn_model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKzQWraVDIeu"
      },
      "outputs": [],
      "source": [
        "# Train model using best arguments for this dataset\n",
        "model, history = train_model(x_train, y_train, 64, 0, 0.001, 32, 100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------\n",
        "# Print model metrics\n",
        "# ------------\n",
        "\n",
        "plot_history(history)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
        "\n",
        "# Calculate AUC\n",
        "auc = metrics.roc_auc_score(y_test, y_pred)\n",
        "print(auc)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
        "plt.title(\"NN\")\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.legend(loc=4)\n",
        "plt.show()\n",
        "\n",
        "# Evaluate loss\n",
        "val_loss = model.evaluate(x_valid, y_valid)\n",
        "print(val_loss)"
      ],
      "metadata": {
        "id": "zN4XF02F07uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing the model"
      ],
      "metadata": {
        "id": "ABoHoZ2-JNNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib"
      ],
      "metadata": {
        "id": "bpxvEuVrJP4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------\n",
        "# Downloading the model\n",
        "# ------------\n",
        "\n",
        "joblib.dump(knn_model, 'aki_diagnoser.joblib')"
      ],
      "metadata": {
        "id": "YnT8uKDUJ7yA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "030c94d7-926b-41e5-84c2-d7ac5b0ace3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aki_diagnoser.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------\n",
        "# Using the model\n",
        "# ------------\n",
        "\n",
        "model = joblib.load('aki_diagnoser.joblib')"
      ],
      "metadata": {
        "id": "EudnryGUMEZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------\n",
        "# Making predictions by using the model\n",
        "# ------------\n",
        "\n",
        "prediction = model.predict([[1.5,11,-1,75]])\n",
        "print(prediction)"
      ],
      "metadata": {
        "id": "ekQQfEdWMYSn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}